{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Model\n",
    "from preprocess import Preprocessor\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: you need to Restart the kernel after changing .py files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "applySmote = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looks like SVM overfits? Specially when C is high."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='color:Orange'>TODO Do we need to normalize data?</div>\n",
    "<div style='color:Orange'>TODO Do we need to one-hot y?</div>\n",
    "\n",
    "<div style='color:Orange'>TODO Do we need regularization? (penalty='l2' in logistic regression?)</div>\n",
    "\n",
    "<div style='color:Orange'>TODO Do we need more models? descision trees? neural networks?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution\n",
      "Before SMOTE:  [150 160 319 552]\n",
      "After SMOTE:  [552 552 552 552]\n",
      "\n",
      "----- svm_linear -----\n",
      " train accuracy: 0.9932065217391305\n",
      " train f1: 0.9932027651341342\n",
      " train precision: 0.9932192599575158\n",
      " train recall: 0.9932065217391304\n",
      "\n",
      " test accuracy: 0.956081081081081\n",
      " test f1: 0.9406308097642165\n",
      " test precision: 0.9409979555328393\n",
      " test recall: 0.9408436930543874\n",
      "\n",
      " time taken: 0.549612283706665 seconds\n",
      "\n",
      "----- logistic_regression -----\n",
      " train accuracy: 0.9180253623188406\n",
      " train f1: 0.9177618607631588\n",
      " train precision: 0.9188882164228185\n",
      " train recall: 0.9180253623188406\n",
      "\n",
      " test accuracy: 0.8412162162162162\n",
      " test f1: 0.8224552026801005\n",
      " test precision: 0.8317466460268318\n",
      " test recall: 0.8297740135267732\n",
      "\n",
      " time taken: 3.3594002723693848 seconds\n",
      "\n",
      "----- linear_regression -----\n",
      " train accuracy: 0.8541666666666666\n",
      " train f1: 0.5754352091490283\n",
      " train precision: 0.588437866730647\n",
      " train recall: 0.5694444444444444\n",
      "\n",
      " test accuracy: 0.7736486486486487\n",
      " test f1: 0.6183059530118353\n",
      " test precision: 0.6293690164928571\n",
      " test recall: 0.6277898969722455\n",
      "\n",
      " time taken: 0.043166399002075195 seconds\n",
      "\n",
      "----- svm_rbf -----\n",
      " train accuracy: 0.8147644927536232\n",
      " train f1: 0.8120436961632083\n",
      " train precision: 0.8217583661063783\n",
      " train recall: 0.8147644927536233\n",
      "\n",
      " test accuracy: 0.7533783783783784\n",
      " test f1: 0.721531916863792\n",
      " test precision: 0.7164575702075702\n",
      " test recall: 0.7486035183627698\n",
      "\n",
      " time taken: 0.29428958892822266 seconds\n",
      "\n",
      "----- naive_bayes -----\n",
      " train accuracy: 0.5892210144927537\n",
      " train f1: 0.5223854641614836\n",
      " train precision: 0.6569530432606887\n",
      " train recall: 0.5892210144927537\n",
      "\n",
      " test accuracy: 0.6081081081081081\n",
      " test f1: 0.46736308048613073\n",
      " test precision: 0.6190113240418118\n",
      " test recall: 0.559196935449958\n",
      "\n",
      " time taken: 0.014094352722167969 seconds\n",
      "\n",
      "----- multinomial_naive_bayes -----\n",
      " train accuracy: 0.7155797101449275\n",
      " train f1: 0.7148272701888838\n",
      " train precision: 0.7217417554165637\n",
      " train recall: 0.7155797101449276\n",
      "\n",
      " test accuracy: 0.652027027027027\n",
      " test f1: 0.5971241986964045\n",
      " test precision: 0.6051367331855136\n",
      " test recall: 0.5931790107232968\n",
      "\n",
      " time taken: 0.018286466598510742 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessor = Preprocessor()\n",
    "df_h = preprocessor.preprocess()\n",
    "X_train, X_test, y_train, y_test = preprocessor.split(df_h)\n",
    "if applySmote:\n",
    "    print('Class distribution')\n",
    "    print('Before SMOTE: ', np.bincount(y_train))\n",
    "    X_train, y_train = preprocessor.SMOTE(X_train, y_train)\n",
    "    print('After SMOTE: ', np.bincount(y_train))\n",
    "    print()\n",
    "\n",
    "model_names = [\n",
    "    'svm_linear',\n",
    "    'logistic_regression',\n",
    "    # gives Recall, f1, precision warning (class not present?)\n",
    "    'linear_regression',\n",
    "    'svm_rbf',\n",
    "    'naive_bayes',\n",
    "    'multinomial_naive_bayes'\n",
    "]\n",
    "C = 10 # default is 1.0\n",
    "\n",
    "s_t = time.time()\n",
    "for model_name in model_names:\n",
    "    \n",
    "    model = None\n",
    "    if model_name == 'svm_linear' or model_name == 'svm_rbf':\n",
    "        model = Model(model_name, C=C)\n",
    "    elif model_name == 'logistic_regression':\n",
    "        model = Model(model_name, max_iter=5000) # to avoid convergence warning, set it high\n",
    "    else:\n",
    "        model = Model(model_name)\n",
    "\n",
    "    model.train(X_train, y_train)\n",
    "\n",
    "    # train accuracy, f1, precision, recall\n",
    "    if model_name == 'linear_regression':\n",
    "        # suppress warning\n",
    "        accuracy, f1, precision, recall = model.evaluate(X_train, y_train, average='macro', zero_division=0)\n",
    "    else:\n",
    "        accuracy, f1, precision, recall = model.evaluate(X_train, y_train, average='macro')\n",
    "    print(f'----- {model_name} -----')\n",
    "    print(f' train accuracy: {accuracy}')\n",
    "    print(f' train f1: {f1}')\n",
    "    print(f' train precision: {precision}')\n",
    "    print(f' train recall: {recall}')\n",
    "    print()\n",
    "\n",
    "    # test accuracy, f1, precision, recall\n",
    "    if model_name == 'linear_regression':\n",
    "        # suppress warning\n",
    "        accuracy, f1, precision, recall = model.evaluate(X_test, y_test, average='macro', zero_division=0)\n",
    "    else:\n",
    "        accuracy, f1, precision, recall = model.evaluate(X_test, y_test, average='macro')\n",
    "    # print(f'----- {model_name} -----')\n",
    "    print(f' test accuracy: {accuracy}')\n",
    "    print(f' test f1: {f1}')\n",
    "    print(f' test precision: {precision}')\n",
    "    print(f' test recall: {recall}')\n",
    "    print()\n",
    "\n",
    "    e_t = time.time()\n",
    "    print(f' time taken: {e_t - s_t} seconds')\n",
    "    print()\n",
    "    s_t = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
